{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Oliver De Sa's Solution**"
      ],
      "metadata": {
        "id": "zHhBlvgyuwqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaboost Implementation\n",
        "\n",
        "In this question, you will finish an implementation of the Adaboost algorithm (using decision stumps as the weak classifier). For your convenience, I have re-written the algorithm steps below:"
      ],
      "metadata": {
        "id": "cYbTBJdmA818"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input**: Data $x^{(n)}$ ($N$ points, $D$ dimensions) and $y^{(n)}$ (N points, binary labels), weak classifier training procedure `weak_learn`, number of training iterations $T$\n",
        "\n",
        "**Output**: $H(x)$, an ensemble of weak classifiers $h_t(x), t = 1, ..., T$ \n",
        "\n",
        "Note: for binary classification, $h_t(x)$ outputs a value in $\\{0,1\\}$\n",
        "\n",
        "1. Initialize sample weights $w^{(n)} = \\frac{1}{N}, n = 1, ..., N$, initialize empty ensemble\n",
        "\n",
        "Repeat steps 2-5 $T$ times:\n",
        "\n",
        "2. Fit a weak classifier $h_t$ to the weighted data $(x^{(n)},y^{(n)},w^{(n)})$, using `weak_learn`\n",
        "\n",
        "3. Compute weighted classification error $$e_t \\leftarrow \\frac{\\sum_{n=1}^N w^{(n)} \\mathbb{I} \\{ h(x^{(n)}) \\neq y^{(n)} \\}}{\\sum_{n=1}^N w^{(n)}}$$ where $\\mathbb{I} \\{ h(x^{(n)}) \\neq x^{(n)} \\}$ is an expression that means \"return 1 if the prediction $h_t(x^{(n)})$ does not equal the true label $y^{(n)}$, otherwise return 0\"\n",
        "\n",
        "4. Compute classifier coefficient $$\\alpha_t \\leftarrow \\frac{1}{2} \\log \\frac{1-e_t}{e_t}$$ Note:  if implemented correctly $\\alpha_t > 0$, the logarithm is base $e$\n",
        "\n",
        "5. Update sample weights $$w^{(n)} \\leftarrow w^{(n)} \\exp(2 \\alpha_t \\mathbb{I} \\{ h(x^{(n)}) \\neq t^{(n)} \\})$$\n",
        "\n",
        "6. Return the ensemble $$H(x) = \\text{sign}\\left(\\sum_{t=1}^{T} \\alpha_t h_t(x)\\right)$$"
      ],
      "metadata": {
        "id": "IdDbM_LUs0CF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In class, we described this algorithm in lecture 5, slide 34."
      ],
      "metadata": {
        "id": "INSl5RUWsy5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in the code below (indicated with the TODO comments). Do not change the functions that have DO NOT MODIFY."
      ],
      "metadata": {
        "id": "g4T2ZGCyxivc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "import sklearn.tree\n",
        "import sklearn.datasets\n",
        "# random seed\n",
        "np.random.seed(1211)"
      ],
      "metadata": {
        "id": "_SBuZKZNpDT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSCwSExso08o"
      },
      "outputs": [],
      "source": [
        "def weak_learn(X,y,w):\n",
        "  \"\"\" \n",
        "  DO NOT MODIFY\n",
        "  train a weak classifier (decision tree with depth of 1)\n",
        "  \"\"\"\n",
        "  \n",
        "  dt = sklearn.tree.DecisionTreeClassifier(max_depth=1)\n",
        "  dt.fit(X,y,w)\n",
        "  return dt\n",
        "\n",
        "def prepare_data():\n",
        "  \"\"\" \n",
        "  DO NOT MODIFY\n",
        "  generate data and randomly split 80/20 train/test\n",
        "  \"\"\"\n",
        "\n",
        "  X, y = sklearn.datasets.make_classification(n_samples=500, n_features=20, n_informative=5)\n",
        "  print(f\"Number of datapoints = {X.shape[0]}\")\n",
        "  print(f\"Number of features = {X.shape[1]}\")\n",
        "  X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,y,test_size=0.2)\n",
        "  return X_train, y_train, X_test, y_test\n",
        "\n",
        "def ensemble_predict(X,ensemble_classifiers,ensemble_alphas):\n",
        "  \"\"\" \n",
        "  DO NOT MODIFY\n",
        "  get predictions for an ensemble\n",
        "  \"\"\"\n",
        "\n",
        "  ensemble_predictions = []\n",
        "  for i in range(len(ensemble_classifiers)):\n",
        "    classifier = ensemble_classifiers[i]\n",
        "    alpha = ensemble_alphas[i]\n",
        "    predictions = classifier.predict(X)\n",
        "    ensemble_predictions.append(alpha*predictions)\n",
        "  ensemble_predictions = np.sum(np.stack(ensemble_predictions,axis=0),axis=0)\n",
        "  return ensemble_predictions > 0.5\n",
        "\n",
        "def ensemble_score(X,y,ensemble_classifiers,ensemble_alphas):\n",
        "  \"\"\" \n",
        "  DO NOT MODIFY\n",
        "  compute accuracy for an ensemble's predictions\n",
        "  \"\"\"\n",
        "\n",
        "  ensemble_predictions = ensemble_predict(X,ensemble_classifiers,ensemble_alphas)\n",
        "  ensemble_accuracy = np.mean(ensemble_predictions==y)\n",
        "  return ensemble_accuracy\n",
        "\n",
        "def run_adaboost(X_train,y_train,num_iters):\n",
        "  \"\"\"\n",
        "  create an ensemble of weighted weak classifiers using the adaboost algorithm\n",
        "  \"\"\"\n",
        "\n",
        "  # data dimensions \n",
        "  # (N is number of training points, D is number of input components)\n",
        "  N = X_train.shape[0]\n",
        "  D = X_train.shape[1]\n",
        "  # TODO initialize weights\n",
        "  weights = np.ones(N)/N\n",
        "  # intialize ensemble\n",
        "  ensemble_classifiers = []\n",
        "  ensemble_alphas = []\n",
        "  for i in range(num_iters):\n",
        "    # TODO train a weak classifier\n",
        "    classifier = weak_learn(X_train,y_train,weights)\n",
        "    # TODO get classifier predictions\n",
        "    predictions = classifier.predict(X_train)\n",
        "    # TODO compute weighted error\n",
        "    weighted_err = np.sum(weights*(predictions != y_train))/np.sum(weights)\n",
        "    # TODO compute classifier coefficient\n",
        "    alpha = 0.5*np.log((1-weighted_err)/weighted_err)\n",
        "    assert alpha >= 0., alpha\n",
        "    # add classifier to ensemble\n",
        "    ensemble_classifiers.append(classifier)\n",
        "    ensemble_alphas.append(alpha)\n",
        "    # TODO compute new weights\n",
        "    weights = weights*np.exp(2*alpha*(predictions != y_train))\n",
        "    weights = weights/np.sum(weights)\n",
        "  return ensemble_classifiers, ensemble_alphas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can test out your implementation here. If you did it correctly, you should get a training accuracy of about 0.64 and a test accuracy of about 0.74"
      ],
      "metadata": {
        "id": "prXiw3hGxuz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = prepare_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBZO2ZXV4I2t",
        "outputId": "a1b983ef-0cdf-443d-ea69-22cc8351bcfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of datapoints = 500\n",
            "Number of features = 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_iters = 5\n",
        "ensemble_classifiers, ensemble_alphas = run_adaboost(X_train,y_train,num_iters)\n",
        "# if you want to test your algorithm, best to restart and run all cells\n",
        "print(ensemble_score(X_train,y_train,ensemble_classifiers,ensemble_alphas)) # should be around 0.64\n",
        "print(ensemble_score(X_test,y_test,ensemble_classifiers,ensemble_alphas)) # should be around 0.74"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmFi23AQwj4v",
        "outputId": "48d29df5-356a-410b-a784-4dd9f9280843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.635\n",
            "0.74\n"
          ]
        }
      ]
    }
  ]
}