{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF_wcrFcZ-9R"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_linear_regression():\n",
        "  \"\"\"\n",
        "  This function sets up a toy linear regression problem and then\n",
        "  solves it analytically and with gradient descent, and prints\n",
        "  both of their solutions.\n",
        "  The two solutions should be quite similar.\n",
        "  \"\"\"\n",
        "\n",
        "  N = 100 # number of data points\n",
        "  D = 2 # dimension of each point\n",
        "  # randomly sample data from gaussian\n",
        "  X = np.random.normal(loc=0.,scale=2.0,size=(N,D)) \n",
        "  # generate targets\n",
        "  true_w = np.random.uniform(low=-1.,high=1.,size=(2,))\n",
        "  y = np.sum(true_w.reshape(1,-1)*X,axis=1)\n",
        "\n",
        "  print(f\"true weights: {true_w}\")\n",
        "  print(f\"analytic optimal weights: {analytic_solution(X,y)}\")\n",
        "  print(f\"gradient descent weights: {gradient_descent_solution(X,y)}\")\n",
        "\n",
        "\n",
        "def gradient_descent_solution(X,y):\n",
        "  \"\"\"\n",
        "  arguments:\n",
        "    X: (N,D) numpy float array of input data\n",
        "    y: (N,) numpy float array of targets\n",
        "  returns:\n",
        "    w: (D,) numpy float array of optimal weights\n",
        "  \"\"\"\n",
        "\n",
        "  # initialize weights and bias randomly for gradient descent\n",
        "  D = X.shape[1]\n",
        "  w = np.random.normal(loc=0.,scale=0.1,size=(D,))\n",
        "  b = np.random.normal(loc=0.,scale=0.1,size=1)\n",
        "  alpha = 0.01 # learning rate\n",
        "  num_iters = 100\n",
        "  for t in range(num_iters):\n",
        "    w = gradient_descent_update(X,y,w,alpha)\n",
        "  return w\n",
        "\n",
        "def compute_gradients(X,y,w):\n",
        "  \"\"\"\n",
        "  Implement this function. Using a for loop (from i=1 to N) is allowed.\n",
        "  Challenge (not for credit): can you think of a vectorized version?\n",
        "  arguments:\n",
        "    X: (N,D) numpy float array of input data\n",
        "    y: (N,) numpy float array of targets\n",
        "    w: (D,) numpy float array of initial weights\n",
        "  returns:\n",
        "    dL_dw: (D,) numpy float array of weight gradients\n",
        "  \"\"\"\n",
        "\n",
        "  N, D = X.shape\n",
        "  dL_dw = (1/N) * X.T @ (X @ w - y)\n",
        "\n",
        "  return dL_dw\n",
        "\n",
        "def gradient_descent_update(X,y,w,alpha):\n",
        "  \"\"\"\n",
        "  Implement this function (using compute_gradients).\n",
        "  arguments:\n",
        "    X: (N,D) numpy float array of input data\n",
        "    y: (N,) numpy float array of targets\n",
        "    w: (D,) numpy float array of initial weights\n",
        "    alpha: float, learning rate\n",
        "  returns:\n",
        "    w: (D,) numpy float array of updated weights\n",
        "  \"\"\"\n",
        "\n",
        "  dL_dw = compute_gradients(X, y, w)\n",
        "  w = w - alpha * dL_dw\n",
        "\n",
        "  return w\n",
        "\n",
        "def analytic_solution(X,y):\n",
        "  \"\"\"\n",
        "  Implement this function (you will need numpy.linalg).\n",
        "  arguments:\n",
        "    X: (N,D) numpy float array of input data\n",
        "    y: (N,) numpy float array of targets\n",
        "  returns:\n",
        "    w: (D,) numpy float array of optimal weights\n",
        "  \"\"\"\n",
        "\n",
        "  X_transpose = np.transpose(X)\n",
        "  X_transpose_X = X_transpose.dot(X)\n",
        "  X_transpose_X_inv = np.linalg.inv(X_transpose_X)\n",
        "  X_transpose_y = X_transpose.dot(y)\n",
        "  w = X_transpose_X_inv.dot(X_transpose_y)\n",
        "\n",
        "  return w"
      ],
      "metadata": {
        "id": "afEdCAPWaA9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_linear_regression()"
      ],
      "metadata": {
        "id": "aiCfR_g_aWUA",
        "outputId": "5f970098-09ed-436b-9100-a37abf13d118",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true weights: [-0.06956962 -0.91422959]\n",
            "analytic optimal weights: [-0.06956962 -0.91422959]\n",
            "gradient descent weights: [-0.06946853 -0.90102785]\n"
          ]
        }
      ]
    }
  ]
}